---
title: "05_relationships"
editor_options: 
  chunk_output_type: console
---

Today we focus on bivariate relationships, we want to measure how 2 variables might be associated one with one another (give exemples of this). Be aware that here, we only talk about relationship but not causality : endogeneity, other variables that might influence a relationship. 


You can do this throught different ways

- Corss-tabulations


## Testing the relationship between two categorial variables : χ² test



You can use the `chisq.test()` function, taking the result of a `table()`. 


## Comparing means 

## Comparing two continous variables with correlations






## Linear regression

- Linear relationship between a continuous variable (our dependent variable) and one or several independent variables. Linear relationship means that this relationship can be represented by a droite. 

In R, it is really easy to calculate a linear regression model. You need to use the `stats::lm()` function. You need to insert a formula with on the left, the variable you want to explain and the right, the independent variables and in between, a `~`. 


## 

```{r}
cor(pr_rn$vote_share, pr_rn$ouvri_share, use = "complete")

model <- lm(vote_share ~ ouvri_share + immig_share, pr_rn)
```

## Extract and interpret the results

Once we have run the model, we first need to extract and interpret the results. In R, you have different ways to look at and to manipulate the results of a regression model. The first one is the `summary()` function.

```{r}
summary(model)
```

First, you have the coefficients. One is the coefficient associated with our independent variable, it is the parameter that the model has estimated, it corresponds to the slope. The other is the intercept, meaning the estimated value of Y when X = 0. You can access directly the coefficient using the `coef()` function

```{r}
coef(model)
```



```{r}
coef(model)
confint(model)
fitted(model)
residuals(model)
```

The summary gives us a lot of information to interpret :

-   Model fit : the R2 is the amount of variation

How we interpret a regression

-   Model fit : R2 : amount of variation of Y that our model is able to explain : goes from 0 to 1. If R2 = 1, our model entirely explains Y (that will never happens). In social science we are happy with R2 of 0.2.

-   Adjusted R2

-   Residual standard error

-   F2

An alternative way to see the results comes from the `broom` package (that you need to install and to load) that has super handy functions to get the results of the regression model in tidy format. That means that you can then easily use them to plot them.

-   Or use the `broom` package that has super handy functions to deal with outpts, with three main functions : `tidy()`, `glance()` and `augment()`

```{r}
broom::tidy(model, conf.int = TRUE)
broom::glance(model)

ggeffects::ggpredict(model, terms = "incumbent") |> plot()
ggeffects::ggpredict(model, terms = "Age_candidat") |> plot()

broom::augment(model) |> 
  ggplot() +
  aes(x = score, y = Age_candidat) +
  geom_point(colour = "blue", alpha = .25) +
  geom_smooth(method = "lm") +
  theme_light()

```

```{r}
ggplot(filter(broom::tidy(model, conf.int = TRUE), term != "(Intercept)")) +
  geom_pointrange(
    aes(
      x = term,
      y = estimate,
      ymin = conf.low,
      ymax = conf.high
    )
  ) +
  geom_hline(yintercept = 0, lty = "dashed") +
  coord_flip() + # invert y/x axes
  theme_bw() +
  labs(
    title = "Bartels model of U.S. presidential electoral outcomes",
    x = NULL # actually modifies the y-axis because of `coord_flip`
  )

ggplot(augment(model, data = pr_rn)) +
  geom_text(aes(y = .std.resid, x = .fitted, label = year)) +
  geom_hline(yintercept = 0, lty = "dashed")

pr_rn |> filter(!is.na(ouvri_share))
```

# Model diagnostics

# ------------------------------------------------------------------------------

## Normality of residuals

Remember that to make valide inference with OLS, the residuals of our model have to be normally distributed.

```{r}
plot(density(residuals(model)))

model |> 
  augment() |> 
  ggplot(aes(.resid)) + 
  geom_rug() + 
  geom_density()
```

```{r}
# residuals-versus-fitted values
plot(residuals(model), fitted(model))

# ... or using `ggplot2`
ggplot(augment(model, data = pr_rn), aes(y = vote_share, x = ouvri_share)) +
  geom_abline(intercept = coef(M1)[1], slope = coef(M1)[2], color = "blue") +
  geom_segment(aes(xend = ouvri_share, yend = .fitted), color = "red") +
  geom_point() +
  geom_label(aes(label = year))
```

```{r}

```

# Step 5: further model diagnostics

# ------------------------------------------------------------------------------

# model performance, using the `performance` package

```{r}
performance::model_performance(model)
```

performance::compare_performance(M1, M2, metrics = "common")

# diagnose possible model issues (equivalent to the `car::vif` function)

```{r}
performance::check_collinearity(model)
```

Outliers

performance::check_outliers(M2)

# ... or, for even more diagnostics plots (requires installing extra packages)

# install.packages(c("see", "patchwork"))

performance::check_model(M2)

# Beta coefficients

# Interactions

# To sum up, what you should learn

-   How to do regression
-   How to extract the output with `broom`
-   How to display and `stargazer`

```{r}
model <- lm(vote_share ~ unemp_share, pr_rn)
```
