---
title: "05_relationships"
editor_options: 
  chunk_output_type: console
---

Today we focus on bivariate relationships, we want to measure how 2 variables might be associated one with one another (give exemples of this). Be aware that here, we only talk about relationship but not causality : endogeneity, other variables that might influence a relationship. When we are interested in inference, we do statistical tests. We usually have a sample of the data and we want to infer something on the population at large. But we need also to measure our uncertainty on this and have a best-guess wich will be our estimate. 

Small data sample : our results depend on how the data was sampled. 

You can do this throught different ways

## Testing bivariate relationships 

Our goal is inference here, we want to ask ourselves, is the relationship we see in the data real or is it due to random chance ? So we postulate a null hypothesis : that the relationship we see is by random chance. We probably want to reject this null hypothesis and prove our alternative hypothesis that this effect is non random. For this we do a test that gives us a p_value which is the probability that our observed data come about if the null hypothesis was true. 

https://www.tidymodels.org/learn/statistics/infer/
https://moderndive.github.io/moderndive_labs/static/previous_versions/v0.6.0/10-hypothesis-testing.html : interesting paragraphs on p-values hacking and misunderstanding them and so on

### Two categorial variables : χ² test

- `table()`, `prop.table()`, `tabyl()`
- Cross tabulations, introduction to kable or gtsummary, exporting table

You can use the `chisq.test()` function, taking the result of a `table()`.

```{r}
library(tidymodels) # Includes the infer package

data(ad_data, package = "modeldata")
ad_data %>%
  select(Genotype, Class)

# calculate the observed statistic
observed_indep_statistic <- ad_data %>%
  specify(Genotype ~ Class) %>%
  calculate(stat = "Chisq")

observed_indep_statistic

null_distribution_simulated <- ad_data %>%
  specify(Genotype ~ Class) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 5000, type = "permute") %>%
  calculate(stat = "Chisq")

null_distribution_simulated

null_distribution_simulated %>%
  visualize() + 
  shade_p_value(observed_indep_statistic,
                direction = "greater")
```



### Comparing means : t.test

### Comparing two continous variables with correlations

```{r}
x <- rnorm(100, 0, 1)
y <- rnorm(100, 0, 2)
bind_rows(
  cor.test(x, y) |> broom::tidy(),
  cor.test(x, y, method = "kendall") |> broom::tidy(),
  cor.test(x, y, method = "spearman") |> broom::tidy()
)

```

- GGaly : ggpair and ggcorr functions with seleft if is.numeric de dplyr

```{r}
library(GGally)
df %>%
dplyr::select_if(is.numeric) %>%
    ggpairs()
```


## Introduction to linear regression

-   Linear relationship between a continuous variable (our dependent variable) and one or several independent variables. Linear relationship means that this relationship can be represented by a droite.

https://jhudatascience.org/tidyversecourse/model.html

In R, it is really easy to calculate a linear regression model. You need to use the `stats::lm()` function. You need to insert a formula with on the left, the variable you want to explain and the right, the independent variables and in between, a `~`.

```{r}

tibble(x =  rnorm(1:100, mean = 0, sd = 1),
       y <- rnorm(1:100, mean = 0, sd = 1)) |>
  ggplot(aes(x, y)) +
  geom_point()

```


## 

```{r}
cor(pr_rn$vote_share, pr_rn$ouvri_share, use = "complete")

model <- lm(vote_share ~ ouvri_share + immig_share, pr_rn)
```

## Extract and interpret the results

Once we have run the model, we first need to extract and interpret the results. In R, you have different ways to look at and to manipulate the results of a regression model. The first one is the `summary()` function.

```{r}
summary(model)
```

First, you have the coefficients. One is the coefficient associated with our independent variable, it is the parameter that the model has estimated, it corresponds to the slope. The other is the intercept, meaning the estimated value of Y when X = 0. You can access directly the coefficient using the `coef()` function

```{r}
coef(model)
```

```{r}
coef(model)
confint(model)
fitted(model)
residuals(model)
```

The summary gives us a lot of information to interpret :

-   Model fit : the R2 is the amount of variation

How we interpret a regression

-   Model fit : R2 : amount of variation of Y that our model is able to explain : goes from 0 to 1. If R2 = 1, our model entirely explains Y (that will never happens). In social science we are happy with R2 of 0.2.

-   Adjusted R2

-   Residual standard error

-   F2

An alternative way to see the results comes from the `broom` package (that you need to install and to load) that has super handy functions to get the results of the regression model in tidy format. That means that you can then easily use them to plot them.

-   Or use the `broom` package that has super handy functions to deal with outpts, with three main functions : `tidy()`, `glance()` and `augment()`

```{r}
broom::tidy(model, conf.int = TRUE)
broom::glance(model)

ggeffects::ggpredict(model, terms = "incumbent") |> plot()
ggeffects::ggpredict(model, terms = "Age_candidat") |> plot()

broom::augment(model) |> 
  ggplot() +
  aes(x = score, y = Age_candidat) +
  geom_point(colour = "blue", alpha = .25) +
  geom_smooth(method = "lm") +
  theme_light()

```

```{r}
ggplot(filter(broom::tidy(model, conf.int = TRUE), term != "(Intercept)")) +
  geom_pointrange(
    aes(
      x = term,
      y = estimate,
      ymin = conf.low,
      ymax = conf.high
    )
  ) +
  geom_hline(yintercept = 0, lty = "dashed") +
  coord_flip() + # invert y/x axes
  theme_bw() +
  labs(
    title = "Bartels model of U.S. presidential electoral outcomes",
    x = NULL # actually modifies the y-axis because of `coord_flip`
  )

ggplot(augment(model, data = pr_rn)) +
  geom_text(aes(y = .std.resid, x = .fitted, label = year)) +
  geom_hline(yintercept = 0, lty = "dashed")

pr_rn |> filter(!is.na(ouvri_share))
```

# Model diagnostics

# ------------------------------------------------------------------------------

## Normality of residuals

Remember that to make valide inference with OLS, the residuals of our model have to be normally distributed.

```{r}
plot(density(residuals(model)))

model |> 
  augment() |> 
  ggplot(aes(.resid)) + 
  geom_rug() + 
  geom_density()
```

```{r}
# residuals-versus-fitted values
plot(residuals(model), fitted(model))

# ... or using `ggplot2`
ggplot(augment(model, data = pr_rn), aes(y = vote_share, x = ouvri_share)) +
  geom_abline(intercept = coef(M1)[1], slope = coef(M1)[2], color = "blue") +
  geom_segment(aes(xend = ouvri_share, yend = .fitted), color = "red") +
  geom_point() +
  geom_label(aes(label = year))
```

```{r}

```

# Step 5: further model diagnostics

# ------------------------------------------------------------------------------

# model performance, using the `performance` package

```{r}
performance::model_performance(model)
```

performance::compare_performance(M1, M2, metrics = "common")

# diagnose possible model issues (equivalent to the `car::vif` function)

```{r}
performance::check_collinearity(model)
```

Outliers

performance::check_outliers(M2)

# ... or, for even more diagnostics plots (requires installing extra packages)

# install.packages(c("see", "patchwork"))

performance::check_model(M2)

# Beta coefficients

# Interactions

# To sum up, what you should learn

-   How to do regression
-   How to extract the output with `broom`
-   How to display and `stargazer`

```{r}
model <- lm(vote_share ~ unemp_share, pr_rn)
```
