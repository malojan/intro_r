---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Introduction to linear regression

As you may have learned in your quantitative methods class, correlation often falls short, and linear regression is frequently employed. Linear regression is considered a cornerstone in quantitative social sciences. It's safe to say that publishing quantitative results can be challenging without utilizing this or a more sophisticated model. Chi-square tests or t-tests are rarely seen published in journals. However, it's essential to recognize that regression has its limitations. Whether you're a quantitative researcher aiming to employ it or a qualitative researcher seeking to comprehend a quantitative article relevant to your work, understanding how regression operates is crucial. When dealing with a sole dependent variable, we refer to it as simple linear regression, which is our primary focus today. In a future session, we will delve into multiple linear regression, involving more than one independent variable.

In linear regression, our objective is to establish a linear relationship between one or more independent variables and a *continuous* dependent variable. However, when working with a dependent variable that is not continuous, other models become relevant, and these will be explored in the upcoming semester. One such model is logistic regression, which is employed when the dependent variable is binary (e.g., whether individuals have voted for Trump or not).

To understand how it works, we will use the electoral results from the last presidential elections at the department level in France, which I have combined with socio-demographic data from the National Statistics Institute (INSEE). My primary focus is to investigate the relationship between the share of workers in a department and the share of votes for the radical right candidate, Marine Le Pen. Previous research has indicated that workers tend to vote disproportionately for the radical right. Therefore, my hypothesis posits that there is a positive relationship between the share of workers and the share of votes for Marine Le Pen. Our regression equation for each departement ${i}$ is written below.

$$
lepenvote_{i} = \beta_{0} + \beta_{1}workershare_{i} + \epsilon_{i}
$$ The goal of the regression is to estimate the parameters of our model wich are $\beta_{0}$,  $\beta_{1}$ and $\epsilon_{i}$.

-   $\beta_{0}$ corresponds to the intercept of the regression line. It provides the estimated value of the vote share for Marine Le Pen when the share of workers in a department is 0. In practical terms, this intercept may or may not have a meaningful interpretation depending on the context of your data. For example, if the share of workers cannot be 0 in reality, the intercept might not hold a direct practical interpretation.
-   $\beta_{1}$ represents the slope of the regression line. It indicates the estimated change in the share of votes for Marine Le Pen for a one-unit change in the share of workers in a department. This coefficient is of particular interest as it demonstrates the strength and direction of the relationship between the two variables.
-   $\epsilon_{i}$ is the error term

This regression model is usally called OLS (ordinary least square)

-   OLS is a certain kind of method of linear model in which we choose the line which has the least prediction errors. This means that it is the best way to fit a line through all the residuals with the least errors. It minimizes the sum of the squared predicition errors

This might still sound abstract to you so let's look at the data ! I first import the data.

```{r warning=FALSE, message=FALSE}

needs(tidyverse, here)
prelec <-read_csv(here("data", "data_pr.csv"))

prelec_dep <- prelec |> 
  filter(candidate %in% c("le_pen")) |> 
  group_by(departement, candidate) |> 
  summarise(vote_share = mean(vote_share, na.rm = TRUE), 
            ouvri_share = mean(ouvri_share, na.rm =TRUE), 
            immig_share = mean(immig_share, na.rm = TRUE)) |> 
  ungroup()
```

First of all, look at this scatter plot between our two variables. We see from this plot that there seems to be a linear relationship.

```{r}
#| warning: false
#| message: false

(rn_ouvri <- prelec_dep |> 
  ggplot(aes(ouvri_share, vote_share)) + 
  geom_point(alpha = 0.5) +
  theme_minimal()) 
```

The goal of linear regression is to find the best-fitting straight line that represents the relationship between two variables based on the given data points. The "best fit" line is determined by minimizing the distance between the line and the actual data points. This distance is measured using residuals, which are the vertical distances from each data point to the line. The line that results in the smallest total residuals provides the best approximation of the relationship between the variables. For example, if we examine the plot below, we can observe that the red and blue lines do not fit the data points well, whereas the black line represents a better fit.

```{r}
#| warning: false
#| message: false

rn_ouvri     +
  geom_abline(
    intercept = 25,
    slope = 0,
    color = "blue",
    linetype = "dashed",
    size = 1,
    alpha = 0.4
  ) +
  geom_abline(
    intercept = 0,
    slope = 1.5,
    color = "red",
    linetype = "dashed",
    size = 1,
    alpha = 0.4
  ) +
  geom_smooth(
    method = "lm",
    linetype = "dashed",
    color = "black",
    se = FALSE,
    size = 1,
    alpha = 0.4
  )
```

Linear regression not only provides us with the best-fitting line but also offers valuable information about the uncertainty associated with the estimated model parameters. This uncertainty is typically represented by confidence intervals around the regression line, which help us understand the range of possible values for the coefficients. Below, you can see this with the grey area around our regression line.

```{r}
#| warning: false
#| message: false

rn_ouvri +
  geom_smooth(method = "lm", linetype = "dashed", color = "black", se = TRUE, size = 1, alpha = 0.4)

rn_ouvri +
  ggrepel::geom_text_repel(aes(label = departement))
```

## Running a regression in R

Now we have a better idea of what a regression is, let's run our first model. In R it is really easy to do ! You need to use the `stats::lm()` function. You need to insert a formula with on the left, the variable you want to explain and the right, the independent variables and in between, a `~`. Before running the model, we have to make sure to know whether we have missing values or not in our data because the model will just drop them out and it is better to be aware of this.

```{r}

prelec_dep <- prelec_dep |> 
  select(ouvri_share, vote_share, departement) |> 
  drop_na()
model <- lm(vote_share ~ ouvri_share, prelec_dep)
```

## Extract and interpret the results

Once we have run the model, we first to extract and interpret the results. In R, you have different ways to look at and to manipulate the results of a regression model. Let's look at them step by step.

### Coefficients

First, you have the coefficients of the model that we have estimated. You can access directly the coefficient using the `coef()` function. An alternative way to see the results comes from the `broom` package (that you need to install and to load) that has super handy functions to get the results of the regression model in tidy format. The `tidy()` function allows you to extract the coefficient of the models and to get their confidence intervals and p values.

```{r}
coef(model)
confint(model)

library(broom)

(tidied <- tidy(model, conf.int = TRUE))
# Add confidence intervals 
```

\|\> mutate(significant_05 = p.value \< 0.5)

-   Our estimate for the intercept is 10.19. This means that when the share of workers in a department is 0, the estimated vote share for Marine Le Pen is around 10.

-   Our estimate for the share of workers in 1.25. This is the slope of the regression line. That means that an increase of 1 percentage point of the share of workers in a departement is associated with an increase of 1.25 point of percentages of the vote share for Marine Le Pen.

-   *statistic* is the t.test of our coefficient/standard error : `1.25398/0.1513677 = 8.28433` : calculate how many standard errors the coefficient is away from 0. If the t.test is \> 1.96, the coefficient will by statistically significant at the 5% level.

-   p.value :`2 * (1 - pt(abs(8.284329), 94))`, df being the degree of freddom which our number of observations - our number of coefficients (96-2)

-   Looking at the p-value, we see that both of the p-values are inferior to 0.05, showing that our p-values are stastically significant for this level of confidence.

You can also use the output of `tidy()` to create coefficient plots. This is especially useful when you have many coefficients, and you want to quickly see which ones are significant or not. If the confidence interval bar crosses the dashed 0 line, it indicates that our coefficient will not be statistically significant. Remember that the sizes of coefficients are not directly comparable because they are not calculated on the same scale. The magnitude of each coefficient depends on the units of the corresponding independent variable. Therefore, comparing coefficients without considering their scales could lead to misleading interpretations. If you are lazy, you can use like [`coeffpot`](https://lrberge.github.io/fixest/reference/coefplot.html), [`dotwhisker`](https://cran.r-project.org/web/packages/dotwhisker/vignettes/dotwhisker-vignette.html), [`ggstat`](https://larmarange.github.io/ggstats/reference/ggcoef_model.html) to create these plots. However, I personally favor creating them manually because it offers much more flexibility.

```{r}
tidied |>
  ggplot(aes(x = term, y = estimate)) +
  # Create dot for our estimates with a bar for confidence intervals
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high))  +
  # Create a dashed line at 0
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_flip() + # invert y/x axes
  theme_minimal()
```

### Model fit

When we run a regression, we also care about how good is our model to explain the variation of our outcome variable. The $R^{2}$ is the amount of variation of our dependent variable that our model is able to explain. It goes from 0 to 1. If the $R^{2}$ = 1, our model entirely explains Y (that will never happens). In social sciences we are happy with $R^{2}$ of 0.2...Here it is `0.4220013` which is quite high.

```{r}
broom::glance(model)
```

If you want all of these information in the same place, you can use the `summary()` function. It is good to have a quick idea of the results. However, it is not suitable for further manipulation of your regression outputs.

```{r}
summary(model)
```

One last useful function from the `broom` package is `augment()`. This will give you the predictions of our model (our fitted values) and our residuals (the errors). This is useful to calculate different diagnostics that we will see in a further session.

```{r}
augment(model)
```

`augment()` is also useful to combine your regression results with your initial dataframe and do some graphs of your results. Here, I plot the regression line

```{r}
model |>
  augment(prelec_dep) |>
  ggplot(aes(x = ouvri_share, y = vote_share, label = departement)) +
  geom_point(colour = "black", alpha = .25) +
  geom_smooth(method = "lm",
              color = "black",
              linetype = "dashed") +
  geom_segment(aes(xend = ouvri_share, yend = .fitted),
               alpha = 0.5,
               color = "blue") +
  ggrepel::geom_text_repel() +
  theme_minimal()
```

The blue segments on the plot are the so-called residuals. These are the vertical distance between the 'true' value of our data to their fitted values on the regression line. The model is fitted by trying to minimize the sum of all of this distances.

With this visualization, we can alo say something about how our model is doing to predict certain points. For instance : Paris, hauts de Seine.

### A note on categorical variables

Here we look at the relationship between two continuous variable. For the example, let's pretend that our variable on workers is a dichotomous one when it is 1 where the

```{r}

prelec_dep <- prelec_dep |> 
  mutate(worker_dummy = if_else(ouvri_share > mean(ouvri_share, na.rm = T), 1, 0) |> as.factor())

prelec_dep |> count(worker_dummy)

model2 <- lm(vote_share ~ worker_dummy, prelec_dep)

tidy(model2)
```

Here : workerdummy1 is to compare to worker dummy0 : being above the average rather than below increases the vote share of 7 point of percentage

We can also compare the fit our two models with glance using the map_df function from the purrr package that is useful to apply a same function to different element of a list and return a tibble.

```{r}
map_df(list(model, model2), glance, .id =  'model')
```

## Going futher : runnning many models at the same time

```{r}
prelec_models <- prelec |> 
  nest(data = - candidate) |> 
  mutate(model = map(data, ~ lm(vote_share ~ ouvri_share, .)), 
         tidied = map(model, ~ tidy(.x, conf.int = TRUE)), 
         glanced = map(model, glance), 
         augmented = map(model, augment))
```

```{r}
prelec_models |> 
  unnest(tidied) |> 
  filter(term == "ouvri_share") |> 
  ggplot(aes(fct_reorder(str_to_title(candidate), estimate), estimate)) + 
  geom_pointrange(
    aes(
      ymin = conf.low,
      ymax = conf.high
    )
  ) +
  geom_hline(yintercept = 0, lty = "dashed") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Candidate", y = "Estimate")

prelec_models |> 
  unnest(glanced) |> 
  ggplot(aes(fct_reorder(str_to_title(candidate), adj.r.squared), adj.r.squared)) + 
  geom_col() +
  coord_flip() +
  labs(x = "Candidate", y = "Adjusted R squared") +
  theme_minimal()
  
prelec_models |> 
  unnest(augmented) |> 
  ggplot() +
  aes(x = ouvri_share, y = vote_share) +
  geom_point(colour = "black", alpha = .25) +
  geom_smooth(method = "lm", color = "blue", linetype = "dashed") +
  theme_light() +
  facet_wrap(~ candidate)

#  geom_segment(aes(xend = agri_share, yend = .fitted), alpha = 0.5, color = "blue") +

```
