---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Multivariate relationships

In practice, linear regression is rarely performed with only one variable but instead with multiple variables. The purpose of using multiple variables is not solely to understand the effect of one variable on another, but rather to "isolate" this effect by controlling for other independent variables that may also contribute to explaining the variance of the dependent variable. This concept is commonly known as *ceteris paribus*, which means keeping all other factors constant.

In R, to conduct a linear regression while controlling for multiple independent variables, you can use the `lm()` function and specify the formula as follows: `lm(y ~ x + z + w, data)`. In this formula, y represents the dependent variable, while x, z, and w represent the independent variables you want to include in the model. The data argument is used to specify the dataset from which the variables are taken.

To learn how to conduct multiple regression in R, we will work with the European Social Survey. We will investigate the question of the gender gap in climate policy attitudes. Is there any gender difference in the way people support climate policies? Part of the literature [@bush2023facing] suggests this, and we want to test whether it is true using European Social Survey data and examine how this varies across countries.

If you are interested in climate policy attitudes, there is nothing in the ESS before wave 6. They now start to include more and more questions about it.

$$
cpsupport_{i} = \alpha + \beta_{0}gender_{i} + \beta_{1}income_{i} + \beta_{3}education_{i} + \beta_{4}leftright_{i} + \beta_{4}age{i} + \epsilon_{i}
$$

```{r}
needs(tidyverse, janitor, labelled, broom, here, haven)

ess <- haven::read_dta(here("data", "ess8.dta"))
```

```{r}
ess |> 
  select(banhhap, inctxff, sbsrnen) |> 
  map(val_labels)
```

```{r}
ess |> 
  select(banhhap, inctxff, sbsrnen) |> 
  skimr::skim()
```

## Recoding the variable of interests

```{r}
ess |> tabyl(stfeco)
ess_recode <- ess |>
  mutate(

    gender = case_match(gndr,
                        1 ~ "Male",
                        2 ~  "Female") |> as.factor(),
    left_right = case_when(is.na(lrscale) ~ 5,
                           TRUE ~ lrscale) |> as.integer(),
    stfeco = case_when(is.na(stfeco) ~ 5,
                        TRUE  ~ stfeco |> as.integer()),
    income = as.integer(hincfel),
    age = 2016 - yrbrn
  )
```

## Runnning the model

```{r}

ess |> colnames( )
ess |> 
  select(banhhap, inctxff, sbsrnen)

ess_fr <- ess_recode |> filter(cntry == "PL")
model <- lm(sbsrnen ~ gender + left_right + income + age + eduyrs + stfeco, ess_recode)

summary(model)

ggeffects::ggpredict(model, terms = c("left_right", "gender")) |> plot()
ggeffects::ggpredict(model, terms = c("stfeco")) |> plot()
```

```{r}
#| output: asis
stargazer::stargazer(model, 
          title = "Model Results", 
          align = TRUE, 
          type = "text",
          
          dep.var.labels = "Position on Multiculturalism",
          covariate.labels = c("Lag of Multic",
                               "Radical Right Support at t-1",
                               "Share of Immigration",
                               "Government Participation",
                               "Party Size")
          )
```

# Running many models

```{r}
ess_models <- ess_recode |>
  select(cntry,
         gender,
         left_right,
         income,
         age,
         eduyrs,
         inctxff,
         banhhap,
         sbsrnen, 
         stfeco) |>
  pivot_longer(
    cols = c(inctxff, banhhap, sbsrnen),
    names_to = "policy",
    values_to = "opposition"
  ) |>
  nest(data =  -c(cntry, policy)) |>
  mutate(
    model = map(
      data,
      ~ lm(opposition ~ gender + left_right + income + age + eduyrs + stfeco, .x)
    ),
    tidied = map(model, ~ tidy(.x, conf.int = T)),
    glanced =  map(model, glance),
    augmented =  map(model, augment)
  )

ess_models |> 
  unnest(tidied)
```

```{r}
ess_models |> 
  unnest(tidied) |> 
  filter(term == "genderMale")|> 
  mutate(significant = p.value < 0.05) |> 
  ggplot(aes(fct_reorder(cntry, estimate), estimate, color = significant)) + 
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high), position = position_dodge(width = 1)) +
  coord_flip() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_light() +
  labs(x = "Country") +
  scale_colour_viridis_d() +
  facet_wrap(~ policy)

ess_models |> 
  unnest(tidied) |> 
  filter(term == "stfeco")|> 
  mutate(significant = p.value > 0.05) |> 
  ggplot(aes(fct_reorder(cntry, estimate), estimate, color = significant)) + 
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high), position = position_dodge(width = 1)) +
  coord_flip() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_light() +
  labs(x = "Country") +
  scale_colour_viridis_d() +
  facet_wrap(~ policy)
```

```{r}
ess_models |>
  unnest(tidied) |>
  filter(term == "left_right") |>
  mutate(
    significant = p.value > 0.05,
    policy = case_match(
      policy,
      "banhhap" ~ "Ban sale non efficient households",
      "inctxff" ~ "Tax fossil fuels",
      "sbsrnen" ~ "Subsidies renewable"
    )
  ) |>
  ggplot(aes(fct_reorder(cntry, estimate), estimate, color = significant)) +
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high), position = position_dodge(width = 1)) +
  coord_flip() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_light() +
  labs(x = "Country") +
  scale_colour_viridis_d() +
  facet_wrap( ~ policy)
```

```{r}
ess_models |> 
  unnest(glanced) |> 
  ggplot(aes(fct_reorder(cntry, adj.r.squared), adj.r.squared)) + 
  geom_col() +
  coord_flip() +
  facet_wrap(~ policy)
```

## PLotting the coefficeints

When we have different variables : useful to plot the different coefficients.

-   You can extract the outputs of model with broom and use them to plot the results
-   Interpretation for cateforical : do not forget refernce

Different packages can also do this more automatically for you and you can have a look on

-   ggcoeff du package ggstats
-   broom helpers marginal predictions

## Model diagnostics

## Normality of residuals

Remember that to make valide inference with OLS, the residuals of our model have to be normally distributed.

```{r}
plot(density(residuals(model)))

model |> 
  augment() |> 
  ggplot(aes(.resid)) + 
  geom_rug() + 
  geom_density()
```

```{r}
# residuals-versus-fitted values
plot(residuals(model), fitted(model))
```


Once we've fit our model, however, we need to perform several diagnostics to see if the specification is right and if we're not breaking various underlying assumptions we make about the data when we use regression. Here, we use the `performance` package to check different of these assumptions.

```{r}
needs(performance)
```

### Multicolinearity

-   Check how our predictors are correlated : because if there are correlated, part of their variance capture the same phenomenon
-   Measuring variance inflation factor VIF : measure how a predictor is independent of each other : you can do this with the `vif()` function from the `car` package.
-   SE could increase due to correlation with other terms
-   Vif should be below ten
-   Measure of tolerance is 1/VIF
-   One way to deal with this = PCA
-   McElreath 2020 : p.169 : not pairwise correlation the problem but condition association (what does this means ? )

One way in R to check for multicolinearity is from the `check_collinearity()` function from the `performance` package (equivalent to the `car::vif` function). The functions calculate the VIF, give confiance interval and calculte tolerance.

-   VIF less than 5 : low correlation between 1 predictor and the others

-   VIF more than 10 is high/not tolerable

```{r}
map(ess_models, ~ check_collinearity(model))
```

### Is colinearity a real problem ?

If it's useful to check for collinearity, be aware that the extent to which this is an important issue is open to debate. Here are some ressources that you could visit if you want to know more about this debate :

https://statisticalhorizons.com/multicollinearity/ : Paul Allison

https://janhove.github.io/analysis/2019/09/11/collinearity, - McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan. 2nd edition. Chapman and Hall/CRC.

## Heteroscedatiscity

-   Variance of errors is not constant, affect estimation of SE and p-values
-   PLot fitted vs residuals
-   performance package allow also to run a Breush_agan test doing an hypothesis of non constant error.
-   With the see package : plot can be done

```{r}

x <- check_heteroscedasticity(model) 

# plot(x)

```

-   Statistical test with ncv test from the car package. if the p-values is smaller than 0.5, we fail to reject the hypothesis of constant variance and so model heteroscedatstic
-   Solution : calculate robust standard errors, can be done with Rcurl or sandwich package and coeftest function : coeff same but se differents

## Outliers

To detect outliers, we use the cook distance which is an outlier detection methods. It estimate how much our regression coefficients change if we remove each observation.

```{r}
check_outliers(model)
```
